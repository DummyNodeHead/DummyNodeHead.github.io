# 预备知识

本小节主要是对pytorch进行相关教程。

## 数据操作

### 张量

张量，即tensor。表示一个由数值组成的数组。这个数组可能有多个维度。具有一个轴的张量称为vector，两个轴称为matrix。两个轴以上没有特殊名称

张量在pytorch中可以被pytorch用来自动构建计算图，是前馈和bp的基础。



### arange

该方法用于创建一个行向量。传入的参数p为行向量的长度，创建出的行向量为 0 ～ p-1 组成的向量，它们被默认创建为浮点数。

```python
x=torch.arange(12)

>>> tensor([0,1,2,…,11])
```



### shape

可以通过张量的shape**属性**（不是方法！）来访问张量的形状，即每个轴的长度。接上文：

```python
x.shape

>>> torch.Size([12])
```



### numel

获取张量中的元素总数的方法。即各个轴的长度乘积。

```python
x.numel()

>>> 12
```



### reshape

改变张量的形状的方法。其中的参数从左到右依次表示：第一个轴有p1个第二个轴的元素，第二个轴有p2个第三个轴的元素……

```python
x=torch.arange(24)
x.reshape(2,3,4)

>>>
	tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```

同时我们可以把参数设置为-1来让该方法自动推断这个位置应该是什么值。比如

```python
x.reshape(-1,3,4)
```

但只能对一个维度设置-1。因为多个-1你无法推断出来。



### zeros

生成全0张量，传入的参数为张量的形状。如：

```python
torch.zeros((2,3,4)) 
# torch.zeros(2,3,4) is ok too

>>> 
	tensor([[[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]],
          
        [[0., 0., 0., 0.],
         [0., 0., 0., 0.],
         [0., 0., 0., 0.]]])
```



### randn

生成随机数张量。当我们初始化神经网络的参数时我们经常会使用随机初始化参数值来进行。使用方法同zeros。

```python
torch.randn(3,4)

>>>
	tensor([[-0.1957, -0.0284,  1.0563,  0.2526],
        [-0.4205,  0.2026,  0.2376, -0.3273],
        [-0.6187,  0.3032, -0.0050, -0.6948]])
```



### tensor

我们还可以通过tensor方法生成张量，传入的参数是每个位置对应的值。可以理解为根据数组生成张量。如：

```python
torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])

>>>
	tensor([[2, 1, 4, 3],
        	[1, 2, 3, 4],
					[4, 3, 2, 1]])
```





## 运算

### 基本运算

对于任意具有相同形状的张量，常见的标准算数运算符（+ - * / **）都可以被升级为按元素运算。即在两个形状相同的张量中，对同一位置的元素进行计算。

包括求幂运算也可以按元素方式进行。如：`torch.exp(x)`。

张量和一个标量做加法或乘法，张量的每个元素都会做这个运算。



### 张量连接

我们可以把多个张量通过cat方法连接在一起。需要传入张量列表以及连哪个轴连接：

```python
X = torch.arange(12, dtype=torch.float32).reshape((3, 4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)

>>>
	(tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),
   
   
 tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))
```



### 张量的关系运算

关系运算也被升级为元素级别。如：

```python
X == Y

>>>
	tensor([[False, True, False, True], 
          [False, False, False, False],
					[False, False, False, False]])
```



### sum

对张量的元素求和返回的不是数值，是只有一个元素的张量：

```python
X.sum()

>>> tensor(66.)
```



### 广播机制

在对于形状不同的张量进行计算时，我们也可以通过广播机制来执行按元素操作。这种机制主要是通过复制元素来扩展张量，使得两个张量有相同的形状，然后对扩展之后对张量按元素操作。

✎ 个人觉得这种机制还是少用为好，降低了代码的可读性，让计算过程晦涩难懂。但它存在总有自己的原因，之后深入学习之后在这方面应该会有更多的理解。张量与标量的加法应该就是广播机制的一种应用。

```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a + b

>>>
	tensor([[0],
         	[1],
         	[2]])
 
	tensor([[0, 1]])
  
  tensor([[0, 1],
        	[1, 2],
					[2, 3]])
```



### 索引和切片

张量中的元素可以通过索引访问。我们可以用 ：来访问指定范围的元素。`[x:y]` 表示的范围是 $[x,y)$。单独的 ：表示这个轴上的所有元素。访问的元素也是张量。

```python
x=torch.tensor([1,2,3])
x[:]=6
x[1:3]

>>> tensor([6,6,6])
		tensor([6,6])

```



### 内存节省

运行一些操作可能会导致新结果重新分配内存。如 `y=x+y`，我们会取消引用y指向的张量，指向新分配的内存处的张量。

这种情况有时可能是不可忍受的。在我们更新参数的时候，如果多次分配内存，会导致算法效率大大减小。此外，如果存在其他引用，当我们更新了引用之后，其他引用可能还会指向旧的位置。

如何原地计算呢？

```python
# 1.使用+=
x += y

# 2.使用切片
z[:] = x + y
```



#### 类型转换

这里的类型转换指的是张量和标量之间的相互转换。前面说过，使用张量会进行计算图的构造。不小心使用张量进行一些中间计算可能会导致这部分被加入到计算图中，从而导致我们bp得到的梯度出错。所以在其他时刻尽量使用标量。

可以通过item方法或者python内置函数将张量转化为python标量：

```python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)

>>> (tensor([3.5000]), 3.5, 3.5, 3)
#						张					    都是标
```





## 数据预处理

本小节主要介绍pandas预处理原始数据并将其转化为张量的步骤。书之后的章节会介绍更多数据预处理技术。

这小节不是很重要。



### csv的读写

写文件没什么好说的，直接贴例子了：

```python
import os

os.makedirs(os.path.join('..', 'data'), exist_ok=True) 
data_file = os.path.join('..', 'data', 'house_tiny.csv') 

with open(data_file, 'w') as f:
  
	f.write('NumRooms,Alley,Price\n') # 列名 

	f.write('NA,Pave,127500\n') # 每行表示一个数据样本 
	f.write('2,NA,106000\n') 
	f.write('4,NA,178100\n')

```



读文件可以通过pandas进行：

```python
import pandas as pd

data = pd.read_csv(data_file)
print(data)



>>> NumRooms Alley   Price
     		NaN  Pave  127500
     		2.0   NaN  106000
     		4.0   NaN  178100
     		NaN   NaN  140000
```



### 处理缺失值

NaN表示缺失。典型的处理方法有插值和删除。插值为插入新值替换缺失值，删除则忽略缺失值。就不展开了。



### 转化为张量

可以利用pandas读取数据的values属性传入`torch.tensor`帮助其转化为张量：

```python
import torch
x = torch.tensor(inputs.values)
```





## 线性代数

这部分主要是线代相关的知识，就不详细展开了，只对部分有意义的地方进行展开。



### 张量的维度

维度这个词在不同上下文往往会有不同的含义。

✦ 向量或轴的维度表示向量或轴的长度，即**元素数量**。

✦ 张量的维度用来表示张量具有的**轴数**。



### 矩阵转置

pytorch中张量的T属性表示它的转置。

```python
x.T
```



### 降维与计算

在`sum(),mean(),average()`中，我们可以传入`axis=<number>`来表示对哪一维进行求和。这是降维的一种方法。

```python
a_sum = A.sum(axis=0)
```

此外，可以再传入`keepdims=True`来保证进行运算但不降维。

```python
a_sum = A.sum(axis=0, keepsdims=True)

>>>
	tensor([[ 6.],
        	[22.],
        	[38.],
        	[54.],
					[70.]])
```



### 点积

两个向量相同位置按元素乘积的和。即：
$$
X^TY=\sum_i^dx_iy_i
$$
在pytorch中点积的计算：

```python
torch.dot(x,y)
```



### 向量积

对于 $A\in R^{m \times n}$ 和向量 $x\in R^n$ ，它们的向量积 $Ax$ 为:

```python
torch.mv(A,x)
```

矩阵的列维度必须与x维度相同。



### 矩阵乘法

```python
torch.mm(A,B)
```



### 范数

一个向量的范数告诉我们一个向量有多大。这里考虑的大小概念不涉及维度，而涉及分量。

**范数的性质：**

1. 按常数因子 $\alpha$ 缩放向量所有元素，其范数也会按相同常数因子的绝对值缩放。

$$
f(\alpha x)=|\alpha|f(x)
$$

2. 满足三角不等式。

$$
f(x+y)\leq f(x)+f(y)
$$

3. 非负。
   $$
   f(x)\geq0
   $$

4. 



$L_2$范数计算:

```python
torch.norm(vector)
```

除了 $L_2$范数外， pytorch中没有其他范数的直接计算方式，需要自己完成。





## 微分

这小节也不详细展开，仅对部分重要内容简单阐述。



### 梯度

梯度是一个多元函数对所有变量的偏导数组成的向量。下列性质经常用到：

​	假设x为n维向量：
$$
A\in R^{m\times n},\nabla Ax=A^T
$$

$$
A\in R^{n\times m},\nabla x^TA=A
$$

$$
A\in R^{n\times n},\nabla x^TAx=(A+A^T)x
$$

$$
\nabla ||x||^2=\nabla x^Tx=2x
$$

同样，对于任何矩阵X，我们也有 $\nabla ||X||^2_F=2X$。





## 自动求导



### requires_grad

在pytorch中，把要求导的张量的`requires_grad`设为True，就可以把它加入进计算图构造。同时，那些根据它计算得出的其他变量也会参与梯度计算。计算的梯度保存在张量的grad属性中。

```python
x=torch.arange(4, requires_grad=True)
y=2*x		# y也会有梯度
z=y.sum()

x.grad	# None
z.backward()	# bp
x.grad # 2
```



### 清空梯度

grad.zero()用于清空梯度。例子如下：

```python
# 接上一节的代码

x.grad.zero()	# 清空梯度，否则后面再对关于x的东西做bp
							# 新梯度会加到老梯度中。如下例：
# eg
b=x.sum()
b.backward()
x.grad	# 理论上应该是1，但实际是3，因为加了之前的2
```



### 分离计算

有时，我们希望将一些计算移动到计算图之外。比如下例，假如我们想在计算 $\nabla _xz$ 时把 $y$ 视为一个常数，那我们需要使用detach()创建一个分离计算图的中间变量。

```python
x.grad.zero()
y=x*x
z=y*x

# 分离
u=y.detach()
z=u*x
z.sum().backward()

x.grad == u # True
```




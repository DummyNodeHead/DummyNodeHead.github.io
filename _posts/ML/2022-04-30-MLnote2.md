---
layout:     post
title:      "《Machine Learing》笔记 2"
subtitle:   "Note for《Machine Learing》 2"
date:       2022-04-30 22:24:00
author:     "Zcy"
header-img: "img/post-bg-css.jpg"
header-mask: 0.4
catalog: false
mathjax: true
tags:
  - 机器学习
  - 笔记
---

说实话，这章的内容感觉有点硬核，而且有的部分作者也似乎没有展开讲清楚。所以理解起来还是有些吃力。

## 过拟合与欠拟合

在实际应用中，我们期望我们的模型能够在新样本上表现的很好。所以我们应该控制模型尽可能从训练样本中学出适用于所有潜在样本的规律。但是，我们不能把训练样本学的“太好”。由于训练样本只是总样本空间的一部分，如果过度训练，模型很可能会把这一部分自己的一些特点也当作了所有样本的一般性质学习下来。这种情况就是过拟合。过拟合往往在训练集上有着极高的精度，但在测试集里的错误率却明显上升。

欠拟合与过拟合相反，即模型学习能力太低没有很好地学到一般规律。欠拟合相比过拟合更加容易克服。比如增加训练轮数，加深网络层数等都可以帮助解决欠拟合。但过拟合就更难解决了，而且过拟合是无法彻底避免的，我们能做的只有“缓解”。



## 评估方法

通常我们通过实验测试来对学习器的繁华误差进行评估并做出选择，为此，我们需要一个测试集开测试模型的泛化能力，然后取测试集上的测试误差作为泛化误差的近似。

在构造测试集时，样本要尽可能不出现在训练集中。假设数据集$D$有$m$个样例，要从中产生出训练集$S$和测试集$T$，常见的做法有：

### 留出法（hold-out）

直接将数据集$D$划分为两个互斥的集合，一个作为$S$，一个作为$T$。

需注意的时，S和T的划分要尽可能保持数据分布的一致性，避免因数据划分引入额外偏差而对最终结果产生影响。例如，在分类任务中要保持样本的类别比例相似，在D中，正例和反例分别为500个，那么在S和T中也要保证二者的比为1:1。通常，我们可以采用**分层抽样**。若S T中样本类别比例差别很大，模型会由于它们分布的差异产生偏差。

单次留出法得到的估计结果往往不够稳定可靠，在实际中一般采用若干次随机划分，重复进行实验评估后取平均值作为留出法的评估结果。

#### 留出法的窘境

我们希望评估的是用$D$训练出的模型性能，但留出法需划分$S$和$T$。这就会导致：若S包含绝大多数样本，则训练出的模型可能更接近于D，但是由于T较小，评估结果可能不够稳定准确。若令T多包含一些样本，那么被评估的模型和用D训练出的模型相比可能有较大差别，降低了结果的保真性(fidelity)。

这个问题没有完美的解决方案，通常将大约$\frac{2}{3}$~$\frac{4}{5}$的样本用于训练，其他用于测试。



### 交叉验证法（cross-validation）

将D划分为k个大小相似的互斥子集，每个子集保证数据分布一致性（分层采样）。然后每次用k-1个子集的并集作为S，余下那个作为T。这样可以获得k组S/T，可以进行k次训练测试。

交叉验证法的评估结果的稳定性和保真性很大程度取决于k。一般k可以取10，20，5。

#### 特殊：留一法（Leave-One-Out LOO）

若k=m，那么T就只有一个样本。这样就使得绝大多数情况下，留一法的模型和D训练出的模型很相似。

留一法的结果往往被认为比较准确（但不是一定），但是中数据集比较大时，训练m个模型开销太大了。



### 自助法（bootstrapping）

有没有办法减少训练样本规模不同造成的影响，同时比较高效地进行实验估计呢？

每次从D中挑选一个样本，拷贝放入D'中，然后再将该样本放回D中进行重复采样。重复m次后，我们就得到了包含m个样本的数据集D'。

样本在m次采样中始终不被采到的概率是$(1-\frac{1}{m})^m$。取极限：

$\lim \limits_{m\rightarrow\infty}(1-\frac{1}{m})^m=\frac{1}{e}\approx0.368$

所以D中有约36.8%的样本未出现在D'中，于是可以用D'作为训练集，D/D'用作测试集。

自助法在数据集小、难以有效划分训练集和测试集时很有用，而且能产生多个不同训练集，对集成学习等方法有很大好处。但是它产生的数据集**改变了初始数据集的分布**，会引入估计偏差。在初始数据量足够时，留出法和交叉验证法更常用。



### 最终模型

在模型选择完成后，我们应该用总数据集D重新训练一次模型，这个模型才是最终使用的模型。

另外，我们把模型在实际使用中遇到的数据称为测试数据，上文用于评估测试的数据集称为**“验证集 (validation set)”**



## 性能度量

### 错误率与精度

对于模型f，错误率定义为：

$E(f;D)=\frac{1}{m}\sum^m_{i=1}\Vert(f(x_i)\neq y_i)$

$\Vert(f(x_i)\neq y_i)$是指如果$f(x_i)\neq y_i$则等于0，否则等于1。

更一般地，对于概率密度函数$p$ ：

$E(f;D)=\int_{x\sim D}\Vert(f(x)\neq y)p(x)dx$

精度即：

$acc(f;D)=1-E(f;D)$



### 查准率，查全率与F1

<img src="/img/in-post/didl/PR.png">

$P=\frac{TP}{TP+FP}$		即预测的所有正类中真正正类的比例

$R=\frac{TP}{TP+FN}$		即预测的正确正类占所有真实正类的比例

查准率和查全率是一对矛盾的度量。一般来说，查准率高时，查全率往往偏低；而查全率高时，查准率往往偏低。

我们通过实验可以做出PR曲线来进行比较。如果一个模型的P-R曲线被另一个模型的曲线完全“包住”，则可断言后者性能优于前者。而如果两个模型的曲线有交叉，则只能在具体的查准率或查全率条件下进行比较。

但是我们对一个算法的评估，不可能单单只考虑某一个方面的性能，所以我们**需要综合考虑准确率和召回率的性能度量**。而且人们仍希望把两个模型比出个高低，于是就有了**平衡点 (Break-Event Point，简称BEP)**和**F1度量**。

BEP很简单，就是P=R的点，比较这几个点与原点之间的欧氏距离得到的大小，越大的越优。

但BEP还是有点太简单了，于是常用的还是F1度量：

$F1=\frac{2\times P \times R}{P+R}$

在一些应用中，对查准率和查全率的重视程度不同。比如在商品推荐系统中更希望推荐用户感兴趣的，此时查准率更重要。而在逃犯信息检索中，要尽量减少漏掉逃犯，查全率更重要。所以我们有了$F_{\beta}$ ，能让我们表达出对查准率/查全率的不同偏好：

$F_{\beta}=\frac{(1+\beta)^2\times P \times R}{(\beta^2 \times P)+R}$

其中$\beta>0$度量了查全率对查准率的相对重要性，$\beta=1$时退化为标准$F1$，$\beta>1$时更看重查全率，反之更看重查准率。

#### macro与micro

但很多时候我们有多个二分类混淆矩阵，比如我们进行了多次训练测试，每次都得到了一个混淆矩阵。那么如何在这么多矩阵上综合考察P和R？

一种直接的做法是先在各混淆矩阵上分贝计算出查准率和查全率，记作$(P_i,R_i)$，再计算平均值：

$macro-P=\frac{1}{n}\sum_{i=1}^nP_i$

$macro-R=\frac{1}{n}\sum_{i=1}^nR_i$

$macro-F1=\frac{2\times macro-P \times macro-R}{macro-P+macro-R}$

我们还可以先把混淆矩阵对应的元素进行平均，计算出TP，FP，TN，FN的平均值，然后基于平均值计算出micro-P, mirco-R, micro-F1，从而进行比较。



### ROC 与 AUC

<img src="/img/in-post/didl/ROC.png">

ROC（Receiver Operating Characteristic）曲线，与P-R曲线相似。我根据机器学习的预测结果对阳历进行排序，如：最可能是正例的排在前，最不可能的排在后。然后计算：

$TPR=\frac{TP}{TP+FN}$

$FPR=\frac{FP}{TN+FP}$

然后分别以它们为横、纵轴作图，得到ROC曲线。现实生活中获得的(FPR, TPR)通常是有限的，所以难以画出光滑曲线。通常都是先将分类阈值设为最大并在原点处标记第一个点，然后以次设为每个样例的预测值（即以次将每个样例划分为正例）。

假设前一个点的坐标是(x,y)，$m^+，m^-$分别表示正例和反例的数量：

若当前例为真正例，则对应标记点的坐标为$(x,y+\frac{1}{m^+})$

若当前例为假正例，则对应标记点的坐标为$(x+\frac{1}{m^-},y)$

最后依次连接点，得到ROC图。

要比较两个模型哪个好和P-R图十分相似，都是包住原则。如果一定要比较出曲线交叉的模型的优劣，则比较ROC曲线下的面积，即AUC（Area Under ROC Curve）。AUC就是传统的算矩形面积求和即可。



### 代价敏感错误率与代价曲线

前面我们考虑的都是代价相等的情况，但生活中我们希望模型出错的代价尽量小。比如若把健康人误诊成病人，会让病人进一步检查，造成麻烦；而若把病人误诊成健康人，那代价可就大了。

为了权衡不同类型错误造成的不同损失，我们赋予错误“非均等代价” (unequal cost)。

在非均等代价下，我们希望的不再是简单地减少错误次数，而是减少总体代价。令$D^+和D^-$分别表示正例和反例，则“代价敏感 (cost-sensitive)”错误率为：

$E(f;D;cost)=\frac{1}{m}(\sum \limits_{x^i\in D^+}\Vert(f(x_i)\neq y_i)\times cost_{01}+\sum \limits_{x^i\in D^-}\Vert(f(x_i)\neq y_i)\times cost_{10})$

以上只是二分类的情况，也可以定义处多分类任务的代价敏感性能度量。

#### 代价曲线的解释

这部分书中只有短短一段，在此说一下我浅薄的理解。

<img src="/Users/zhaocanyu/浙江大学/大三下/_posts/img/in-post/didl/CC.png">

首先，代价曲线的横轴为：（其实这个东西的意义还不是很懂）

$P(+)=\frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}$			（1）

纵轴为：

$cost_{norm}=\frac{FNR\times p \times cost_{01}+FNR \times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}$		（2）

把(1)式带入(2)，则有：

$cost_{norm}=FNR \times P(+)+FPR\times (1-P(+))$

所以CC曲线就是cost关于P(+)的曲线，每个FNR和FPR对应一条直线。

当我们给定不同阈值时，会获得多组(FNR, FPR)，于是就能获得多条直线。

对于P(+)，我们可以理解为是我们把样本预测为正例需要付出的代价。在P(+)处做一条垂线，会有多条直线与它相交。但是最下面那条就是代价最小的。而我们有了最小代价的模型，肯定不会再去考虑代价大的，所以最小代价的直线围城的面积就可以表示这个模型期望的总体代价。





## 比较检验

统计假设检验(hypothesis test)为我们进行模型性能比较提供了重要依据。我们可以推断出若验证集上A比B好，那么A的泛化性能是否在统计学意义上优于B。本节默认错误率为性能度量，用$\epsilon$表示。

### 假设检验

泛化错误率与测试错误率未必相同，但直观上二者接近的可能性比较大，相差很远的可能性比较小，因此可根据测试错误率推断泛化错误率的分布。

### 交叉验证t检验

若两个学习器的性能相同，则它们使用相同的训练/测试集得到的测试错误率应相同。在显著度$\alpha$下，若变量$\tau_t=\vert\frac{\sqrt{k}u}{\sigma}\vert$小于临界值$t_{\alpha/2,k-1}$，则假设不能被拒绝，即认为两个学习器的性能没有显著差别。

通常由于样本有限，使用交叉验证法不同轮次的训练集会有一定重叠，使得测试错误率实际上并不独立，会导致过高估计的假设成立概率。为缓解这一问题可以采用“5 x 2交叉验证法”。

“5 x 2交叉验证法”即做5次2折交叉验证，在每次2折交叉验证之前随机将数据打乱，使得5次交叉验证中的数据划分不重复。

### McNemar检验

对二分类问题，我们可以列出列联表(contingency table)。根据卡方分布来判断。

### Friedman检验与Nemenyi后续检验

Friedman：在不同数据集上对算法进行比较。根据算法在每个数据集上的测试性能由好到坏排序，然后求出平均序值。

Nemenyi：计算出平均序值差别的临界值域CD，若两个算法的平均序值之和超出了CD，则拒绝两个算法性能相同的假设。

## 偏差与方差

偏差-方差分解(bias-variance decomposition)是解释学习算法泛化性能的一种重要工具。

偏差度量了学习算法的期望预测与真实结果的偏离程度，刻画了学习算法本身的拟合能力。

方差度量了同样大小的训练集的变动所导致的学习性能变化，刻画了数据扰动所造成的影响。

噪声表示了学习问题本身的难度。

**偏差-方差窘境（bias-variance dilemma）：**在训练不足时，模型的拟合能力不够强，训练数据的扰动不足以让学习起产生显著变化，此时偏差主导了泛化错误率。随着训练程度的加深，模型的拟合能力逐渐增强，训练数据发生的扰动渐渐能被模型学到，方差逐渐主导了泛化错误率。在训练充分后，模型的拟合能力已经非常强，若训练数据自身的非全局的特性被模型学到，那就会发生过拟合。
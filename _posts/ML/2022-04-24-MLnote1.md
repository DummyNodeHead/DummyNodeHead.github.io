---
layout:     post
title:      "《Machine Learing》笔记 1"
subtitle:   "Note for《Machine Learing》 1"
date:       2022-04-24 17:13:00
author:     "Zcy"
header-img: "img/post-bg-css.jpg"
header-mask: 0.4
catalog: false
mathjax: true
tags:
  - 机器学习
  - 笔记
---



## 基本术语

1. dataset

   用于进行机器学习的数据集合。

   

2. instance/sample

   dataset中的一条记录。

   

3. attribute/feature

   每条记录的某个属性。

   

4. attribute space/sample space

   属性张成的空间。

   

5. feature vector

   在属性张成的空间中，每个sample可以在空间中找到自己的坐标位置。空间中每个点对应一个坐标向量，所以每个sample又称为一个feature vector。

   

6. dimensionality

   每条记录由多少个属性描述。

   

7. hypothesis

   机器学习是在寻找某种规律，起初不知道这个规律是什么，于是会针对潜在规律进行假设。

   

8. ground-truth

   潜在规律本身。

   

9. regression

   预测连续值的任务。

   

10. clasification

    预测离散值的任务。包括binary和multi-class。

    

11. clustering

    在一些任务中是无法得到label的。ML需要自己寻找数据的特征。针对数据是否有label，学习任务又可分为supervised learning和unsupervised learning。

    

12. generalization

    模型适用于新样本的能力。





## 假设空间

假设的表示一旦确定，那么假设空间机器规模大小就确定了。由输入到输出这个映射的集合被称为假设空间(hypothesis space)。与训练数据一致的假设组成的集合被称为版本空间(version space)。

以书中西瓜例子为例，要根据色泽、根蒂、敲声来判断是否是好瓜，假设每个属性有3种取值，则假设空间的大小一共为

$4\*4\*4+1=65$

即3种情况+1种无论什么值都可以的情况，每个属性共可以取4个值。如果根本不存在好瓜这个概念，则为空集，所以空集也要加上，最后假设空间大小为65。





## 归纳偏好

机器学习算法在学习过程中对某种类型假设的偏好被称为归纳偏好(inductive bias)。任何一个有效的机器学习算法必须有归纳偏好。否则，等效的假设可能会给出不同的结果。比如有许多种判断好瓜的假设，如果算法有归纳偏好，则会从中选择它更相信的假设来给出输出。否则，算法一下使用这个假设，一下使用另一个假设，每次给出的输出结果还要看概率，那机器学习算法就没有任何意义了。



奥卡姆剃刀 (Occam's razor) 是一种常用的偏好建立原则。即“若有多个假设和观察一致，选择最简单的那个”。但在实际应用中，如何定义“简单”也是一个问题。同时，剃刀也非唯一可行的建立原则。

在现实问题中，**算法的归纳偏好是否与问题本身匹配，大多时候直接决定了算法的性能。**



既然有归纳偏好的存在，那么也不难理解以下情况：

```
存在两种假设，分别为h1和h2

若h1在情况A下表现良好
那么必然存在其他情况，h2表现比h1好

这个结论对任何算法均成立，包括“胡乱瞎猜”这样的笨拙算法
```

所以，空谈“什么算法更好”并没有意义。由NFL定理，如果要考虑所有潜在问题，那么所有的学习算法都一样好。要比较算法优劣必须要针对具体的问题来谈。

